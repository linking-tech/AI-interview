{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990f6d3d",
   "metadata": {},
   "source": [
    "# Transformer related code practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b43fc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3f03e",
   "metadata": {},
   "source": [
    "## Softmax Calculation\n",
    "### Formula\n",
    "$$Softmax(i) = \\frac{e^i}{\\sum_{j=1}^{n}e^j}$$\n",
    "\n",
    "### Function\n",
    "#### `np.sum(x, axis=axis, keepdims=True)中的axis参数决定了沿哪个维度进行求和：`\n",
    "\n",
    "axis=0：按列求和，压缩第 0 维（行）。\n",
    "\n",
    "axis=1：按行求和，压缩第 1 维（列）。\n",
    "\n",
    "axis=-1：按最后一维求和（适用于多维数组）。\n",
    "\n",
    "keepdims=True：保持结果的维度与输入一致，仅将求和的维度大小变为 1。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5bf8f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.35153650e-44, 6.39213895e-44, 1.73756352e-43, 4.72318733e-43,\n",
       "       1.28389543e-42, 3.48998961e-42, 9.48677535e-42, 2.57877290e-41,\n",
       "       7.00983153e-41, 1.90546977e-40, 5.17960384e-40, 1.40796230e-39,\n",
       "       3.82723833e-39, 1.04035124e-38, 2.82796788e-38, 7.68721369e-38,\n",
       "       2.08960133e-37, 5.68012532e-37, 1.54401814e-36, 4.19707646e-36,\n",
       "       1.14088367e-35, 3.10124334e-35, 8.43005342e-35, 2.29152610e-34,\n",
       "       6.22901377e-34, 1.69322149e-33, 4.60265322e-33, 1.25113086e-32,\n",
       "       3.40092628e-32, 9.24467611e-32, 2.51296351e-31, 6.83094304e-31,\n",
       "       1.85684283e-30, 5.04742213e-30, 1.37203159e-29, 3.72956853e-29,\n",
       "       1.01380184e-28, 2.75579911e-28, 7.49103864e-28, 2.03627542e-27,\n",
       "       5.53517048e-27, 1.50461533e-26, 4.08996852e-26, 1.11176871e-25,\n",
       "       3.02210068e-25, 8.21492137e-25, 2.23304715e-24, 6.07005148e-24,\n",
       "       1.65001106e-23, 4.48519509e-23, 1.21920243e-22, 3.31413582e-22,\n",
       "       9.00875516e-22, 2.44883355e-21, 6.65661973e-21, 1.80945684e-20,\n",
       "       4.91861366e-20, 1.33701781e-19, 3.63439123e-19, 9.87929963e-19,\n",
       "       2.68547207e-18, 7.29986992e-18, 1.98431037e-17, 5.39391483e-17,\n",
       "       1.46621807e-16, 3.98559393e-16, 1.08339676e-15, 2.94497771e-15,\n",
       "       8.00527940e-15, 2.17606055e-14, 5.91514586e-14, 1.60790335e-13,\n",
       "       4.37073446e-13, 1.18808881e-12, 3.22956021e-12, 8.77885484e-12,\n",
       "       2.38634016e-11, 6.48674509e-11, 1.76328013e-10, 4.79309234e-10,\n",
       "       1.30289758e-09, 3.54164282e-09, 9.62718331e-09, 2.61693974e-08,\n",
       "       7.11357975e-08, 1.93367146e-07, 5.25626399e-07, 1.42880069e-06,\n",
       "       3.88388295e-06, 1.05574884e-05, 2.86982290e-05, 7.80098743e-05,\n",
       "       2.12052824e-04, 5.76419338e-04, 1.56687021e-03, 4.25919482e-03,\n",
       "       1.15776919e-02, 3.14714295e-02, 8.55482149e-02, 2.32544158e-01,\n",
       "       6.32120559e-01])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def softmax(x , axis):\n",
    "\t\"\"\"\n",
    "\tCompute softmax of array x.\n",
    "\t\"\"\"\n",
    "\texp_x = np.exp(x-np.max(x,axis=axis, keepdims=True))\n",
    "\treturn exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "a = [i for i in range(0,101)]\n",
    "print(a)\n",
    "\n",
    "softmax(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62fa34e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0],\n",
       "        [0, 1]]),\n",
       " array([[1, 0],\n",
       "        [0, 1]]),\n",
       " array([[1, 2],\n",
       "        [3, 4]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "\t\"\"\"\n",
    "\tCompute Q, K, V matrices from input X using the provided weight matrices.\n",
    "\t\"\"\"\n",
    "\tQ = np.dot(X, W_q)\n",
    "\tK = np.dot(X, W_k)\n",
    "\tV = np.dot(X, W_v)\n",
    "\n",
    "\treturn Q, K, V\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "compute_qkv(X, W_q, W_k, W_v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b09887",
   "metadata": {},
   "source": [
    "### Self Attention\n",
    "\n",
    "$$Attention(Q, K, V) = Softmax(\\frac{Q K^T}{\\sqrt{d_K}})$$\n",
    "\n",
    "$ Q=XW_q $  &emsp; &emsp; $ K=XW_k $ &emsp; &emsp; $ V=XW_v $\n",
    "在深度学习中，高维矩阵乘法（如批量矩阵乘法）是常见操作，但维度变化容易混淆。以下是详细解释和示例：\n",
    "\n",
    "\n",
    "### **1. 二维矩阵乘法（基础）**\n",
    "对于二维矩阵 $A$ 和 $B$：\n",
    "- $A$ 的形状为 `(m, n)`\n",
    "- $B$ 的形状为 `(n, p)`\n",
    "- 结果 $C = A \\times B$ 的形状为 `(m, p)`\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "A = np.random.randn(3, 4)  # 形状: (3, 4)\n",
    "B = np.random.randn(4, 5)  # 形状: (4, 5)\n",
    "C = np.dot(A, B)          # 形状: (3, 5)\n",
    "```\n",
    "\n",
    "\n",
    "### **2. 高维矩阵乘法（批量矩阵）**\n",
    "在深度学习中，通常需要处理批量数据，例如：\n",
    "- 输入序列形状为 `(batch_size, seq_len, hidden_dim)`\n",
    "\n",
    "此时的矩阵乘法称为**批量矩阵乘法**，需满足：\n",
    "- 最后两个维度满足二维矩阵乘法规则\n",
    "- 前面的维度必须完全相同或为1（广播机制）\n",
    "\n",
    "#### **规则**：\n",
    "假设两个高维矩阵 $A$ 和 $B$：\n",
    "- $A$ 的形状为 `(a₁, a₂, ..., aₖ, m, n)`\n",
    "- $B$ 的形状为 `(a₁, a₂, ..., aₖ, n, p)`\n",
    "- 结果 $C = A \\times B$ 的形状为 `(a₁, a₂, ..., aₖ, m, p)`\n",
    "\n",
    "\n",
    "### **3. NumPy中的批量矩阵乘法**\n",
    "在NumPy中，可以使用以下方式进行批量矩阵乘法：\n",
    "1. **`np.matmul` 或 `@` 运算符**：专门处理批量矩阵乘法\n",
    "2. **`np.einsum`**：更灵活的爱因斯坦求和表示法\n",
    "\n",
    "\n",
    "### **4. 常见场景示例**\n",
    "#### **场景1：自注意力机制中的QKV计算**\n",
    "- $Q$ 的形状：`(batch_size, seq_len, hidden_dim)`\n",
    "- $K$ 的形状：`(batch_size, seq_len, hidden_dim)`\n",
    "- 计算 $QK^T$：\n",
    "  ```python\n",
    "  attention_scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
    "  # Q: (batch_size, seq_len, hidden_dim)\n",
    "  # K.transpose(0, 2, 1): (batch_size, hidden_dim, seq_len)\n",
    "  # 结果: (batch_size, seq_len, seq_len)\n",
    "  ```\n",
    "\n",
    "#### **场景2：多头注意力中的每个头**\n",
    "假设 `num_heads = 8`，`hidden_dim = 512`，则每个头的维度为 `512/8 = 64`：\n",
    "- $Q$ 的形状：`(batch_size, seq_len, num_heads, head_dim)`\n",
    "- $K$ 的形状：`(batch_size, seq_len, num_heads, head_dim)`\n",
    "- 计算 $QK^T$：\n",
    "  ```python\n",
    "  # 调整维度：将num_heads移到batch_size维度前\n",
    "  Q_reshaped = Q.transpose(0, 2, 1, 3).reshape(batch_size*num_heads, seq_len, head_dim)\n",
    "  K_reshaped = K.transpose(0, 2, 1, 3).reshape(batch_size*num_heads, seq_len, head_dim)\n",
    "  \n",
    "  # 批量矩阵乘法\n",
    "  attention_scores = np.matmul(Q_reshaped, K_reshaped.transpose(0, 2, 1))\n",
    "  # 结果形状: (batch_size*num_heads, seq_len, seq_len)\n",
    "  ```\n",
    "\n",
    "\n",
    "### **5. 使用 `einsum` 简化高维运算**\n",
    "`einsum` 可以更清晰地表达复杂的矩阵运算：\n",
    "```python\n",
    "# 自注意力中的QK^T计算\n",
    "attention_scores = np.einsum('bih,bjh->bij', Q, K)\n",
    "# 含义：\n",
    "# b: batch_size\n",
    "# i, j: 序列位置\n",
    "# h: hidden_dim\n",
    "# 结果形状: (batch_size, seq_len, seq_len)\n",
    "\n",
    "# 多头注意力中的QK^T计算\n",
    "attention_scores = np.einsum('bhid,bhjd->bhij', Q, K)\n",
    "# 含义：\n",
    "# b: batch_size\n",
    "# h: num_heads\n",
    "# i, j: 序列位置\n",
    "# d: head_dim\n",
    "# 结果形状: (batch_size, num_heads, seq_len, seq_len)\n",
    "```\n",
    "\n",
    "\n",
    "### **6. 广播机制在矩阵乘法中的应用**\n",
    "如果两个矩阵的某些维度不匹配，但其中一个为1，则会触发广播机制：\n",
    "```python\n",
    "A = np.random.randn(2, 3, 4, 5)  # 形状: (2, 3, 4, 5)\n",
    "B = np.random.randn(2, 1, 5, 6)  # 形状: (2, 1, 5, 6)\n",
    "C = np.matmul(A, B)              # 形状: (2, 3, 4, 6)\n",
    "# B的第二个维度为1，会自动广播为3以匹配A\n",
    "```\n",
    "\n",
    "\n",
    "### **总结**\n",
    "1. **二维矩阵乘法**：`(m, n) × (n, p) → (m, p)`\n",
    "2. **批量矩阵乘法**：最后两维满足二维规则，前面的维度必须相同或为1\n",
    "3. **常用方法**：\n",
    "   - `np.matmul` 或 `@`：处理批量矩阵乘法\n",
    "   - `np.einsum`：灵活表达复杂运算\n",
    "4. **注意转置操作**：如自注意力中的 `K.transpose(0, 2, 1)`\n",
    "5. **广播机制**：当维度为1时自动扩展匹配\n",
    "\n",
    "理解这些规则后，高维矩阵乘法的维度变化将变得清晰可控。\n",
    "\n",
    "- $ W_q \\in \\mathbb{R}^{d \\times d_k}$：查询权重矩阵\n",
    "\n",
    "- $ W_k \\in \\mathbb{R}^{d \\times d_k}$：键权重矩阵\n",
    "\n",
    "- $ W_v \\in \\mathbb{R}^{d \\times d_v}$：值权重矩阵\n",
    "\n",
    "- $ d_k$ 和 $d_v$ 分别是键和值的维度，通常 $d_k = d_v = d/h$（h 是注意力头数）\n",
    "​\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae6dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.53788284, 2.53788284],\n",
       "       [2.46211716, 3.46211716]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def self_attention(Q, K, V):\n",
    "\t\"\"\"\n",
    "\t在没有batch-size维度的情况下, 计算attention可以直接点乘\n",
    "\t\"\"\"\n",
    "\tattention_scores = np.dot(Q, K.T)/np.sqrt(Q.shape[-1])\n",
    "\tattention_weights = softmax(attention_scores, 1)\n",
    "\tattention_output = np.dot(attention_weights, V)\n",
    "\n",
    "def self_attention_with_batch(Q, K, V):\n",
    "\t\"\"\"\n",
    "\t在有batch-size维度的情况下, 计算attention需要将batch-size维度展开\n",
    "\t\"\"\"\n",
    "\t#batch_size维度保持不变，seq_len, d_model维度转置\n",
    "\tattention_scores = np.matmul(Q, K.transpose(0, 2, 1))/np.sqrt(Q.shape[-1])\n",
    "\tattention_weights = softmax(attention_scores, 1)\n",
    "\tattention_output = np.dot(attention_weights, V)\n",
    "\n",
    "\n",
    "\n",
    "\treturn attention_output\n",
    "\n",
    "Q , K , V = compute_qkv(X, W_q, W_k, W_v)\n",
    "\n",
    "self_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b51f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5498\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Activation Function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "\t#Your code here\n",
    "    result = round(1/(1 + np.e**(-z)),4)\n",
    "    return result\n",
    "\n",
    "print(sigmoid(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8955147",
   "metadata": {},
   "source": [
    "这道题目要求实现一个基于梯度下降的线性回归函数，核心目标是通过迭代优化找到最佳的回归系数（包括截距项），使模型对目标值的预测尽可能准确。下面详细解析题意和解决方法：\n",
    "\n",
    "\n",
    "### **一、题意解析**\n",
    "#### **1. 输入与输出**\n",
    "- **输入**：\n",
    "  - `X`：特征矩阵（NumPy数组），其中已包含一列全为1的列（用于表示线性回归中的截距项，即常数项）。假设`X`的形状为`(m, n)`，其中`m`是样本数量，`n`是特征数量（含截距项对应的列）。\n",
    "  - `y`：目标值数组（NumPy数组），形状为`(m,)`，表示每个样本对应的真实值。\n",
    "  - `alpha`：学习率（步长），控制每次参数更新的幅度。\n",
    "  - `iterations`：迭代次数，即梯度下降的总步数。\n",
    "- **输出**：\n",
    "  - 线性回归模型的系数（NumPy数组），形状为`(n,)`，包含截距项和各特征的系数，结果需保留4位小数。\n",
    "\n",
    "#### **2. 核心任务**\n",
    "通过梯度下降算法最小化线性回归的损失函数（均方误差），求解最优参数`θ`（系数向量），使得模型预测值`y_pred = X·θ`尽可能接近真实值`y`。\n",
    "\n",
    "\n",
    "### **二、线性回归与梯度下降的核心原理**\n",
    "#### **1. 线性回归模型**\n",
    "线性回归的数学表达式为：  \n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_{n-1} x_{n-1}$$  \n",
    "其中：\n",
    "- $\\hat{y}$ 是预测值，$\\theta_0$ 是截距项（常数项），$\\theta_1, ..., \\theta_{n-1}$ 是各特征的系数。\n",
    "- 若将特征矩阵`X`扩展为包含一列全为1的列（用于表示$\\theta_0$的系数），则模型可简化为矩阵形式：  \n",
    "  $$\\hat{y} = X \\cdot \\theta$$  \n",
    "  其中`X`的形状为`(m, n)`，`θ`的形状为`(n,)`，$\\hat{y}$的形状为`(m,)`。\n",
    "\n",
    "#### **2. 损失函数**\n",
    "线性回归使用**均方误差（MSE）** 作为损失函数，衡量预测值与真实值的平均偏差：  \n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$  \n",
    "（公式中除以`2m`而非`m`是为了求导后简化计算，不影响最优解）。\n",
    "\n",
    "#### **3. 梯度下降算法**\n",
    "梯度下降是一种迭代优化方法，通过不断沿着损失函数的负梯度方向更新参数，逐步减小损失值。核心步骤如下：  \n",
    "1. **初始化参数**：通常将`θ`初始化为全零向量。  \n",
    "2. **计算梯度**：损失函数对`θ`的梯度（偏导数）为：  \n",
    "   $$\\nabla J(\\theta) = \\frac{1}{m} X^T \\cdot (X \\cdot \\theta - y)$$  \n",
    "   其中`X·θ - y`是预测值与真实值的误差向量。  \n",
    "3. **更新参数**：沿着负梯度方向调整`θ`：  \n",
    "   $$\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)$$  \n",
    "   （`α`为学习率，控制更新幅度）。  \n",
    "4. **迭代优化**：重复步骤2和3，直到达到指定迭代次数，最终得到的`θ`即为最优系数。\n",
    "\n",
    "\n",
    "### **三、解决方法（步骤拆解）**\n",
    "#### **1. 初始化参数**\n",
    "- 系数向量`θ`的长度等于特征矩阵`X`的列数（`n`），初始化为全零向量（`np.zeros(n)`）。  \n",
    "  （注：初始化为零不影响最终结果，因为线性回归的损失函数是凸函数，只有一个全局最优解。）\n",
    "\n",
    "#### **2. 迭代更新参数**\n",
    "对于每一次迭代（共`iterations`次）：  \n",
    "- **步骤1：计算预测误差**  \n",
    "  预测值`y_pred = X·θ`（用矩阵乘法`np.dot(X, θ)`或`X @ θ`实现），误差向量为`error = y_pred - y`。  \n",
    "- **步骤2：计算梯度**  \n",
    "  根据梯度公式，梯度`gradient = (X^T · error) / m`（`X^T`是`X`的转置，`m`是样本数）。  \n",
    "- **步骤3：更新系数**  \n",
    "  按照`θ = θ - α * gradient`更新参数，沿着负梯度方向减小损失。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76216d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1107],\n",
       "       [0.9513]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "\tm, n = X.shape\n",
    "\ttheta = np.zeros((n, 1))\n",
    "\tfor _ in range(iterations):\n",
    "\t\terror = np.dot(X, theta) - y\n",
    "\t\tgradient = np.dot(X.T, error) / m\n",
    "\t\ttheta = theta - alpha * gradient\n",
    "\treturn np.round(theta, 4)\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([[1], [2], [3]])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "linear_regression_gradient_descent(X, y, alpha, iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
